{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e15255f",
   "metadata": {},
   "source": [
    "###### About Data\n",
    "- For our project, we choose dataset from `TripAdvisor` having restaurant data across 7 major states in USA from [`kaggle`](https://www.kaggle.com/datasets/siddharthmandgi/tripadvisor-restaurant-recommendation-data-usa).\n",
    "- This dataset was scraped from the TripAdvisor website and it conatins restaurant data across 20 cities in Washington, Texas, California, New York, Pensi, New Jersey and Oregon and Pennsylvania.\n",
    "\n",
    "*Note:* Our dataset file can be accessible here on [github](https://github.com/sifat-e-noor/Recommendation-system)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09256d03",
   "metadata": {},
   "source": [
    "###### Format and Clean the Dataset\n",
    "\n",
    "For our further analysis to buil a resturant recommendation system, we clean and format the data set as per requirement. \n",
    "\n",
    "To find out issues with our choosen dataset, we will do the followings-\n",
    "- Check for missing values for overall dataset\n",
    "- Check closely for subtle values in other coulmn(s) through unique values except missing values column(s)\n",
    "- Check closely for non-relatable strings except missing values column(s)\n",
    "- Drop NaN values from required row(s)/column(s) \n",
    "- Fix unrelatable data values \n",
    "- Drop non require row(s)/column(s)\n",
    "- Chnage data type (if require)\n",
    "- Reorder column(s) and reset index (if require)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b3e01",
   "metadata": {},
   "source": [
    "##### All important import(s) goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a36b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31012d27",
   "metadata": {},
   "source": [
    "###### Download dataset (.csv) file from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547dca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functin to download file from github\n",
    "def file_exist(file_name):\n",
    "    return file_name in os.listdir(os.getcwd())\n",
    "def download_file(url,file_name):\n",
    "    if file_exist(file_name):\n",
    "        return\n",
    "    response = requests.get(url)\n",
    "    open(file_name, \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c261e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset's first 3 data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Street Address</th>\n",
       "      <th>Location</th>\n",
       "      <th>Type</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>No of Reviews</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Contact Number</th>\n",
       "      <th>Trip_advisor Url</th>\n",
       "      <th>Menu</th>\n",
       "      <th>Price_Range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Betty Lou's Seafood and Grill</td>\n",
       "      <td>318 Columbus Ave</td>\n",
       "      <td>San Francisco, CA 94133-3908</td>\n",
       "      <td>Seafood, Vegetarian Friendly, Vegan Options</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>243 reviews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+1 415-757-0569</td>\n",
       "      <td>https://www.tripadvisor.com//Restaurant_Review...</td>\n",
       "      <td>Check The Website for a Menu</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coach House Diner</td>\n",
       "      <td>55 State Rt 4</td>\n",
       "      <td>Hackensack, NJ 07601-6337</td>\n",
       "      <td>Diner, American, Vegetarian Friendly</td>\n",
       "      <td>4 of 5 bubbles</td>\n",
       "      <td>84 reviews</td>\n",
       "      <td>Both times we were there very late, after 11 P...</td>\n",
       "      <td>+1 201-488-4999</td>\n",
       "      <td>https://www.tripadvisor.com//Restaurant_Review...</td>\n",
       "      <td>Check The Website for a Menu</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Table Talk Diner</td>\n",
       "      <td>2521 South Rd Ste C</td>\n",
       "      <td>Poughkeepsie, NY 12601-5476</td>\n",
       "      <td>American, Diner, Vegetarian Friendly</td>\n",
       "      <td>4 of 5 bubbles</td>\n",
       "      <td>256 reviews</td>\n",
       "      <td>Waitress was very friendly but a little pricey...</td>\n",
       "      <td>+1 845-849-2839</td>\n",
       "      <td>https://www.tripadvisor.com//Restaurant_Review...</td>\n",
       "      <td>http://tabletalkdiner.com/menu/breakfast/</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name       Street Address  \\\n",
       "0  Betty Lou's Seafood and Grill     318 Columbus Ave   \n",
       "1              Coach House Diner        55 State Rt 4   \n",
       "2               Table Talk Diner  2521 South Rd Ste C   \n",
       "\n",
       "                       Location                                          Type  \\\n",
       "0  San Francisco, CA 94133-3908   Seafood, Vegetarian Friendly, Vegan Options   \n",
       "1     Hackensack, NJ 07601-6337          Diner, American, Vegetarian Friendly   \n",
       "2   Poughkeepsie, NY 12601-5476          American, Diner, Vegetarian Friendly   \n",
       "\n",
       "            Reviews No of Reviews  \\\n",
       "0  4.5 of 5 bubbles   243 reviews   \n",
       "1    4 of 5 bubbles    84 reviews   \n",
       "2    4 of 5 bubbles   256 reviews   \n",
       "\n",
       "                                            Comments   Contact Number  \\\n",
       "0                                                NaN  +1 415-757-0569   \n",
       "1  Both times we were there very late, after 11 P...  +1 201-488-4999   \n",
       "2  Waitress was very friendly but a little pricey...  +1 845-849-2839   \n",
       "\n",
       "                                    Trip_advisor Url  \\\n",
       "0  https://www.tripadvisor.com//Restaurant_Review...   \n",
       "1  https://www.tripadvisor.com//Restaurant_Review...   \n",
       "2  https://www.tripadvisor.com//Restaurant_Review...   \n",
       "\n",
       "                                        Menu Price_Range  \n",
       "0               Check The Website for a Menu    $$ - $$$  \n",
       "1               Check The Website for a Menu    $$ - $$$  \n",
       "2  http://tabletalkdiner.com/menu/breakfast/    $$ - $$$  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from git\n",
    "url = \"https://github.com/sifat-e-noor/Recommendation-system/raw/main/Data/TripAdvisor_RestauarantRecommendation.csv\"\n",
    "download_file(url,\"TripAdvisor_RestauarantRecommendation.csv\")\n",
    "df_dataset = pd.read_csv(\"TripAdvisor_RestauarantRecommendation.csv\")\n",
    "\n",
    "# View the dataset's first 3 data\n",
    "print(f\"Our dataset's first 3 data:\")\n",
    "df_dataset.head(3)\n",
    "# View full dataset\n",
    "# df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68537b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset (3062, 11)\n",
      "Size of dataset 33682\n",
      "It has 3062 rows, and 11 columns titled ase:\n",
      "1 Name\n",
      "2 Street Address\n",
      "3 Location\n",
      "4 Type\n",
      "5 Reviews\n",
      "6 No of Reviews\n",
      "7 Comments\n",
      "8 Contact Number\n",
      "9 Trip_advisor Url\n",
      "10 Menu\n",
      "11 Price_Range\n"
     ]
    }
   ],
   "source": [
    "# Obtain basic info of dataset (i.e shape, row and column) \n",
    "print(\"Shape of dataset\", df_dataset.shape)\n",
    "print(\"Size of dataset\", df_dataset.size)\n",
    "print(f\"It has {len(df_dataset.axes[0])} rows, and {len(df_dataset.axes[1])} columns titled ase:\")\n",
    "for i, col in enumerate(df_dataset.axes[1]):\n",
    "    print(i+1, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eadd886",
   "metadata": {},
   "source": [
    "- From the above observation, we found that our dataset has the following attriutes:\n",
    "\n",
    "__1.__ Name (Name of restuarants), __2.__ Streest Address (Name of the Street), __3.__ Location (City and Zip code), __4.__ Type (Types of cuisines served), __5.__ Reviews (Star Ratings), __6.__ No of Reviews (No. of People who have rated), __7.__ Comments (Customer reviews), __8.__ Contact Number (USA - Phone Number), __9.__ Trip_advisor Url, __10.__ Menu URL, and __11.__ Price_Range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315276a",
   "metadata": {},
   "source": [
    "- For our further analysis, We dont need columns from 8 - 10 as these columns values do not contribute in our goal's analysis(i.e 8. Contact Number, 9. Menu url, and 10. Trip Advisor Url). Therefore, to make the dataset more viusally manageable within the screensize, we discard these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71825445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset's first 3 data after dropping out 3 columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Street_Address</th>\n",
       "      <th>Location</th>\n",
       "      <th>Type</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Price_Range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Betty Lou's Seafood and Grill</td>\n",
       "      <td>318 Columbus Ave</td>\n",
       "      <td>San Francisco, CA 94133-3908</td>\n",
       "      <td>Seafood, Vegetarian Friendly, Vegan Options</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>243 reviews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coach House Diner</td>\n",
       "      <td>55 State Rt 4</td>\n",
       "      <td>Hackensack, NJ 07601-6337</td>\n",
       "      <td>Diner, American, Vegetarian Friendly</td>\n",
       "      <td>4 of 5 bubbles</td>\n",
       "      <td>84 reviews</td>\n",
       "      <td>Both times we were there very late, after 11 P...</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Table Talk Diner</td>\n",
       "      <td>2521 South Rd Ste C</td>\n",
       "      <td>Poughkeepsie, NY 12601-5476</td>\n",
       "      <td>American, Diner, Vegetarian Friendly</td>\n",
       "      <td>4 of 5 bubbles</td>\n",
       "      <td>256 reviews</td>\n",
       "      <td>Waitress was very friendly but a little pricey...</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sixty Vines</td>\n",
       "      <td>3701 Dallas Pkwy</td>\n",
       "      <td>Plano, TX 75093-7777</td>\n",
       "      <td>American, Wine Bar, Vegetarian Friendly</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>235 reviews</td>\n",
       "      <td>Not sure why I went there for the second time....</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Clam Bar</td>\n",
       "      <td>3914 Brewerton Rd</td>\n",
       "      <td>Syracuse, NY 13212</td>\n",
       "      <td>American, Bar, Seafood</td>\n",
       "      <td>4 of 5 bubbles</td>\n",
       "      <td>285 reviews</td>\n",
       "      <td>Doesn't look like much from the outside but wa...</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>Indigo Kitchen &amp; Ale House</td>\n",
       "      <td>2902 164th St SW</td>\n",
       "      <td>Lynnwood, WA 98087-3201</td>\n",
       "      <td>American, Bar, Vegetarian Friendly</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>198 reviews</td>\n",
       "      <td>We had to wait a few minutes to get it but it ...</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>City Vineyard</td>\n",
       "      <td>233 West Street</td>\n",
       "      <td>New York City, NY 10013</td>\n",
       "      <td>American, Bar, Seafood</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>374 reviews</td>\n",
       "      <td>Came here to see Andrea Gibson perform, we wer...</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>BRIO Tuscan Grille</td>\n",
       "      <td>3710 US Highway 9</td>\n",
       "      <td>Freehold, NJ 07728-4801</td>\n",
       "      <td>Steakhouse, Italian, Bar</td>\n",
       "      <td>4 of 5 bubbles</td>\n",
       "      <td>211 reviews</td>\n",
       "      <td>We come to Brio once a month. They are good fo...</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>Maywood Pancake house</td>\n",
       "      <td>92 W Pleasant Ave</td>\n",
       "      <td>Maywood, NJ 07607-1336</td>\n",
       "      <td>American, Vegan Options, Gluten Free Options</td>\n",
       "      <td>4 of 5 bubbles</td>\n",
       "      <td>87 reviews</td>\n",
       "      <td>Stopped in yesterday for Breakfast. When in Je...</td>\n",
       "      <td>$$ - $$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>Porto Leggero</td>\n",
       "      <td>Harborside Financial Center - Plaza 5</td>\n",
       "      <td>Jersey City, NJ 07311</td>\n",
       "      <td>Italian, Vegetarian Friendly, Vegan Options</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>183 reviews</td>\n",
       "      <td>First time for dinner. 2 couples. Delicious mu...</td>\n",
       "      <td>$$$$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3062 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Name                         Street_Address  \\\n",
       "0     Betty Lou's Seafood and Grill                       318 Columbus Ave   \n",
       "1                 Coach House Diner                          55 State Rt 4   \n",
       "2                  Table Talk Diner                    2521 South Rd Ste C   \n",
       "3                       Sixty Vines                       3701 Dallas Pkwy   \n",
       "4                      The Clam Bar                      3914 Brewerton Rd   \n",
       "...                             ...                                    ...   \n",
       "3057     Indigo Kitchen & Ale House                       2902 164th St SW   \n",
       "3058                  City Vineyard                        233 West Street   \n",
       "3059             BRIO Tuscan Grille                      3710 US Highway 9   \n",
       "3060          Maywood Pancake house                      92 W Pleasant Ave   \n",
       "3061                  Porto Leggero  Harborside Financial Center - Plaza 5   \n",
       "\n",
       "                          Location  \\\n",
       "0     San Francisco, CA 94133-3908   \n",
       "1        Hackensack, NJ 07601-6337   \n",
       "2      Poughkeepsie, NY 12601-5476   \n",
       "3             Plano, TX 75093-7777   \n",
       "4               Syracuse, NY 13212   \n",
       "...                            ...   \n",
       "3057       Lynnwood, WA 98087-3201   \n",
       "3058       New York City, NY 10013   \n",
       "3059       Freehold, NJ 07728-4801   \n",
       "3060        Maywood, NJ 07607-1336   \n",
       "3061         Jersey City, NJ 07311   \n",
       "\n",
       "                                               Type           Reviews  \\\n",
       "0       Seafood, Vegetarian Friendly, Vegan Options  4.5 of 5 bubbles   \n",
       "1              Diner, American, Vegetarian Friendly    4 of 5 bubbles   \n",
       "2              American, Diner, Vegetarian Friendly    4 of 5 bubbles   \n",
       "3           American, Wine Bar, Vegetarian Friendly  4.5 of 5 bubbles   \n",
       "4                            American, Bar, Seafood    4 of 5 bubbles   \n",
       "...                                             ...               ...   \n",
       "3057             American, Bar, Vegetarian Friendly  4.5 of 5 bubbles   \n",
       "3058                         American, Bar, Seafood  4.5 of 5 bubbles   \n",
       "3059                       Steakhouse, Italian, Bar    4 of 5 bubbles   \n",
       "3060   American, Vegan Options, Gluten Free Options    4 of 5 bubbles   \n",
       "3061    Italian, Vegetarian Friendly, Vegan Options  4.5 of 5 bubbles   \n",
       "\n",
       "     No_of_Reviews                                           Comments  \\\n",
       "0      243 reviews                                                NaN   \n",
       "1       84 reviews  Both times we were there very late, after 11 P...   \n",
       "2      256 reviews  Waitress was very friendly but a little pricey...   \n",
       "3      235 reviews  Not sure why I went there for the second time....   \n",
       "4      285 reviews  Doesn't look like much from the outside but wa...   \n",
       "...            ...                                                ...   \n",
       "3057   198 reviews  We had to wait a few minutes to get it but it ...   \n",
       "3058   374 reviews  Came here to see Andrea Gibson perform, we wer...   \n",
       "3059   211 reviews  We come to Brio once a month. They are good fo...   \n",
       "3060    87 reviews  Stopped in yesterday for Breakfast. When in Je...   \n",
       "3061   183 reviews  First time for dinner. 2 couples. Delicious mu...   \n",
       "\n",
       "     Price_Range  \n",
       "0       $$ - $$$  \n",
       "1       $$ - $$$  \n",
       "2       $$ - $$$  \n",
       "3       $$ - $$$  \n",
       "4       $$ - $$$  \n",
       "...          ...  \n",
       "3057    $$ - $$$  \n",
       "3058    $$ - $$$  \n",
       "3059    $$ - $$$  \n",
       "3060    $$ - $$$  \n",
       "3061        $$$$  \n",
       "\n",
       "[3062 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns form 8-10(i.e 8. Contact Number, 9 Trip_advisor Url, 10 Menu) \n",
    "df_new_dataset = df_dataset.drop(df_dataset.loc[:, 'Contact Number':'Menu'].columns, axis=1)\n",
    "\n",
    "# Rename 2 columns to check those columns' values\n",
    "df_new_dataset.rename(columns = {'Street Address':'Street_Address', 'No of Reviews':'No_of_Reviews'}, inplace = True)\n",
    "\n",
    "# View the dataset's first 3 data\n",
    "print(f\"Our dataset's first 3 data after dropping out 3 columns:\")\n",
    "# df_new_dataset.head(3)\n",
    "df_new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e0c4f",
   "metadata": {},
   "source": [
    "- To find issues (i.e. missing values) in our data set, we get dataset's basic info first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4fda0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic info of dataset:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3062 entries, 0 to 3061\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Name            3062 non-null   object\n",
      " 1   Street_Address  3062 non-null   object\n",
      " 2   Location        3062 non-null   object\n",
      " 3   Type            3049 non-null   object\n",
      " 4   Reviews         3062 non-null   object\n",
      " 5   No_of_Reviews   3062 non-null   object\n",
      " 6   Comments        2447 non-null   object\n",
      " 7   Price_Range     3062 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 191.5+ KB\n",
      "\n",
      "\n",
      "Follwing Columns have missing values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name                0\n",
       "Street_Address      0\n",
       "Location            0\n",
       "Type               13\n",
       "Reviews             0\n",
       "No_of_Reviews       0\n",
       "Comments          615\n",
       "Price_Range         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View dataset's per columns overall basic info\n",
    "print('Basic info of dataset:\\n') \n",
    "df_new_dataset.info()\n",
    "print('\\n')\n",
    "# Find dataset's overall missing values per columns\n",
    "print('Follwing Columns have missing values:')\n",
    "df_new_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5ba9c",
   "metadata": {},
   "source": [
    "- From above table we can see that `Type` and `comments` have 13 and 615 visible missing values. Lets drop these two columns NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60b5a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name              0\n",
       "Street_Address    0\n",
       "Location          0\n",
       "Type              0\n",
       "Reviews           0\n",
       "No_of_Reviews     0\n",
       "Comments          0\n",
       "Price_Range       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NaN values\n",
    "df_new_dataset.dropna(inplace=True, axis=0)\n",
    "df_new_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabba8ad",
   "metadata": {},
   "source": [
    "- Check closely for subtle values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4ed8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name column has: 2101 data\n",
      "Unique values of Name column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Coach House Diner', 'Table Talk Diner', 'Sixty Vines', ...,\n",
       "       'BRIO Tuscan Grille', 'Maywood Pancake house', 'Porto Leggero'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find unique values for each Name column to see there is any non-meaningful string exist\n",
    "print(f'Name column has: {len(df_new_dataset.Name.unique())} data')\n",
    "print(\"Unique values of \"'Name'\" column:\")\n",
    "df_new_dataset.Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8802319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unrelateable data exist in Name column\n"
     ]
    }
   ],
   "source": [
    "# As Name column has 3062 data, we can't review all unique values in bare eyes. \n",
    "# Therefore, we check closely Name column to find unrelateable string data\n",
    "is_no_name = False\n",
    "for name in df_new_dataset.Name:    \n",
    "    if name == 'No name' and name == 'Undefined':\n",
    "        is_no_name = True\n",
    "        print(name)\n",
    "    else:\n",
    "        pass\n",
    "if is_no_name == True:\n",
    "    print(\"Unrelatable data exists\")\n",
    "else:\n",
    "    print(\"No unrelateable data exist in Name column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df456db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street_Address column has: 2231 data\n",
      "Unique values of Street_Address column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Coach House Diner', 'Table Talk Diner', 'Sixty Vines', ...,\n",
       "       'BRIO Tuscan Grille', 'Maywood Pancake house', 'Porto Leggero'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find unique values for Street_Address column to see there is any non-meaningful string exist\n",
    "print(f'Street_Address column has: {len(df_new_dataset.Street_Address.unique())} data')\n",
    "print(\"Unique values of \"'Street_Address'\" column:\")\n",
    "df_new_dataset.Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973d41c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unrelateable data exists in Street_Address column\n"
     ]
    }
   ],
   "source": [
    "# As Street_Address column has 2813 data, we can't review all unique values in bare eyes. \n",
    "# Therefore, we check closely Street_Address column to find unrelateable string data\n",
    "is_no_address = False\n",
    "for address in df_new_dataset.Street_Address:    \n",
    "    if address == 'No address' and address == 'Undefined':\n",
    "        is_no_address = True\n",
    "        print(address)\n",
    "    else:\n",
    "        pass\n",
    "if is_no_address == True:\n",
    "    print(\"Unrelatable data exists\")\n",
    "else:\n",
    "    print(\"No unrelateable data exists in Street_Address column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa5ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location column has: 2103 data\n",
      "Unique values of Location column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Hackensack, NJ 07601-6337', 'Poughkeepsie, NY 12601-5476',\n",
       "       'Plano, TX 75093-7777', ..., 'Freehold, NJ 07728-4801',\n",
       "       'Maywood, NJ 07607-1336', 'Jersey City, NJ 07311'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find unique values for Location column to see there is any non-meaningful string exist\n",
    "print(f'Location column has: {len(df_new_dataset.Location.unique())} data')\n",
    "print(\"Unique values of \"'Location'\" column:\")\n",
    "df_new_dataset.Location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c51016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unrelateable data exist in Location column\n"
     ]
    }
   ],
   "source": [
    "# As Location column has 3062 data, we can't review all unique values in bare eyes. \n",
    "# Therefore, we check closely Location column to find unrelateable string data\n",
    "is_no_location = False\n",
    "for location in df_new_dataset.Location:    \n",
    "    if location == 'No location' and location == 'Undefined':\n",
    "        is_no_location = True\n",
    "        print(location)\n",
    "    else:\n",
    "        pass\n",
    "if is_no_location == True:\n",
    "    print(\"Unrelatable data exists\")\n",
    "else:\n",
    "    print(\"No unrelateable data exist in Location column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b351de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews column has: 5 data\n",
      "Unique values of Reviews column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['4 of 5 bubbles', '4.5 of 5 bubbles', '5 of 5 bubbles',\n",
       "       '3.5 of 5 bubbles', '3 of 5 bubbles'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find unique values for Reviews column to see there is any non-meaningful string exist\n",
    "print(f'Reviews column has: {len(df_new_dataset.Reviews.unique())} data')\n",
    "print(\"Unique values of \"'Reviews'\" column:\")\n",
    "df_new_dataset.Reviews.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9e203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase readability, we remove redundent words from Reviews column \n",
    "df_new_dataset['Reviews'] = [n.split()[0] for n in df_new_dataset['Reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "258278e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No_of_Reviews column has:693 data\n",
      "Unique values of No_of_Reviews column:\n",
      "['84 reviews' '256 reviews' '235 reviews' '285 reviews' '220 reviews'\n",
      " '89 reviews' '90 reviews' '258 reviews' '271 reviews' '87 reviews'\n",
      " '118 reviews' '706 reviews' '104 reviews' '314 reviews' '1,198 reviews'\n",
      " '280 reviews' '642 reviews' '928 reviews' '30 reviews' '746 reviews'\n",
      " '119 reviews' '515 reviews' '123 reviews' '189 reviews' '39 reviews'\n",
      " '742 reviews' '111 reviews' '24 reviews' '1,653 reviews' '223 reviews'\n",
      " '153 reviews' '240 reviews' '191 reviews' '198 reviews' '29 reviews'\n",
      " '214 reviews' '513 reviews' '44 reviews' '307 reviews' '660 reviews'\n",
      " '413 reviews' '28 reviews' '74 reviews' '160 reviews' '273 reviews'\n",
      " '86 reviews' '932 reviews' '378 reviews' '217 reviews' '437 reviews'\n",
      " '138 reviews' '51 reviews' '100 reviews' '219 reviews' '184 reviews'\n",
      " '11 reviews' '58 reviews' '12 reviews' '1,490 reviews' '493 reviews'\n",
      " '54 reviews' '25 reviews' '516 reviews' '282 reviews' '202 reviews'\n",
      " '114 reviews' '108 reviews' '47 reviews' '77 reviews' '166 reviews'\n",
      " '161 reviews' '165 reviews' '373 reviews' '116 reviews' '210 reviews'\n",
      " '132 reviews' '404 reviews' '46 reviews' '393 reviews' '545 reviews'\n",
      " '488 reviews' '95 reviews' '379 reviews' '171 reviews' '203 reviews'\n",
      " '65 reviews' '269 reviews' '181 reviews' '783 reviews' '182 reviews'\n",
      " '441 reviews' '262 reviews' '64 reviews' '236 reviews' '350 reviews'\n",
      " '201 reviews' '310 reviews' '332 reviews' '805 reviews' '238 reviews'\n",
      " '3,274 reviews' '130 reviews' '563 reviews' '162 reviews' '144 reviews'\n",
      " '141 reviews' '185 reviews' '150 reviews' '241 reviews' '209 reviews'\n",
      " '80 reviews' '206 reviews' '137 reviews' '428 reviews' '82 reviews'\n",
      " '311 reviews' '224 reviews' '43 reviews' '40 reviews' '395 reviews'\n",
      " '146 reviews' '317 reviews' '216 reviews' '218 reviews' '239 reviews'\n",
      " '362 reviews' '1,123 reviews' '85 reviews' '21 reviews' '443 reviews'\n",
      " '354 reviews' '55 reviews' '821 reviews' '211 reviews' '564 reviews'\n",
      " '244 reviews' '56 reviews' '270 reviews' '670 reviews' '1,633 reviews'\n",
      " '290 reviews' '13 reviews' '205 reviews' '204 reviews' '319 reviews'\n",
      " '42 reviews' '1,045 reviews' '267 reviews' '170 reviews' '147 reviews'\n",
      " '351 reviews' '2,518 reviews' '1,076 reviews' '27 reviews' '369 reviews'\n",
      " '179 reviews' '148 reviews' '377 reviews' '70 reviews' '53 reviews'\n",
      " '178 reviews' '1,117 reviews' '187 reviews' '374 reviews' '666 reviews'\n",
      " '834 reviews' '386 reviews' '942 reviews' '19 reviews' '157 reviews'\n",
      " '253 reviews' '175 reviews' '194 reviews' '232 reviews' '32 reviews'\n",
      " '113 reviews' '485 reviews' '72 reviews' '37 reviews' '88 reviews'\n",
      " '274 reviews' '276 reviews' '59 reviews' '106 reviews' '384 reviews'\n",
      " '712 reviews' '260 reviews' '498 reviews' '22 reviews' '167 reviews'\n",
      " '370 reviews' '172 reviews' '229 reviews' '69 reviews' '215 reviews'\n",
      " '324 reviews' '163 reviews' '231 reviews' '723 reviews' '36 reviews'\n",
      " '176 reviews' '521 reviews' '287 reviews' '121 reviews' '180 reviews'\n",
      " '49 reviews' '281 reviews' '133 reviews' '76 reviews' '398 reviews'\n",
      " '151 reviews' '207 reviews' '322 reviews' '356 reviews' '15 reviews'\n",
      " '304 reviews' '438 reviews' '419 reviews' '94 reviews' '1,381 reviews'\n",
      " '112 reviews' '125 reviews' '695 reviews' '26 reviews' '246 reviews'\n",
      " '937 reviews' '200 reviews' '230 reviews' '75 reviews' '544 reviews'\n",
      " '23 reviews' '136 reviews' '284 reviews' '1,021 reviews' '302 reviews'\n",
      " '254 reviews' '233 reviews' '186 reviews' '1,578 reviews' '1,129 reviews'\n",
      " '16 reviews' '829 reviews' '463 reviews' '752 reviews' '283 reviews'\n",
      " '110 reviews' '48 reviews' '91 reviews' '61 reviews' '225 reviews'\n",
      " '20 reviews' '272 reviews' '83 reviews' '406 reviews' '221 reviews'\n",
      " '96 reviews' '1,548 reviews' '159 reviews' '349 reviews' '747 reviews'\n",
      " '129 reviews' '134 reviews' '861 reviews' '190 reviews' '372 reviews'\n",
      " '156 reviews' '1,406 reviews' '1,085 reviews' '323 reviews' '115 reviews'\n",
      " '303 reviews' '659 reviews' '264 reviews' '247 reviews' '197 reviews'\n",
      " '417 reviews' '618 reviews' '392 reviews' '353 reviews' '424 reviews'\n",
      " '605 reviews' '574 reviews' '594 reviews' '876 reviews' '102 reviews'\n",
      " '60 reviews' '396 reviews' '188 reviews' '966 reviews' '222 reviews'\n",
      " '73 reviews' '301 reviews' '2,282 reviews' '71 reviews' '45 reviews'\n",
      " '1,439 reviews' '243 reviews' '394 reviews' '298 reviews' '345 reviews'\n",
      " '263 reviews' '357 reviews' '557 reviews' '342 reviews' '684 reviews'\n",
      " '35 reviews' '1,458 reviews' '278 reviews' '292 reviews' '973 reviews'\n",
      " '109 reviews' '807 reviews' '503 reviews' '131 reviews' '3,807 reviews'\n",
      " '477 reviews' '648 reviews' '99 reviews' '34 reviews' '1,237 reviews'\n",
      " '234 reviews' '843 reviews' '173 reviews' '249 reviews' '62 reviews'\n",
      " '728 reviews' '510 reviews' '326 reviews' '691 reviews' '535 reviews'\n",
      " '305 reviews' '255 reviews' '363 reviews' '1,512 reviews' '537 reviews'\n",
      " '265 reviews' '505 reviews' '329 reviews' '2,617 reviews' '623 reviews'\n",
      " '120 reviews' '174 reviews' '780 reviews' '694 reviews' '140 reviews'\n",
      " '266 reviews' '904 reviews' '128 reviews' '251 reviews' '1,096 reviews'\n",
      " '177 reviews' '364 reviews' '584 reviews' '1,564 reviews' '66 reviews'\n",
      " '383 reviews' '1,182 reviews' '849 reviews' '248 reviews' '169 reviews'\n",
      " '103 reviews' '1,360 reviews' '299 reviews' '252 reviews' '983 reviews'\n",
      " '321 reviews' '433 reviews' '226 reviews' '418 reviews' '50 reviews'\n",
      " '490 reviews' '312 reviews' '308 reviews' '2,534 reviews' '432 reviews'\n",
      " '306 reviews' '117 reviews' '139 reviews' '142 reviews' '533 reviews'\n",
      " '228 reviews' '242 reviews' '135 reviews' '561 reviews' '68 reviews'\n",
      " '149 reviews' '647 reviews' '3,095 reviews' '895 reviews' '2,306 reviews'\n",
      " '158 reviews' '154 reviews' '426 reviews' '1,657 reviews' '107 reviews'\n",
      " '152 reviews' '336 reviews' '4,620 reviews' '536 reviews' '748 reviews'\n",
      " '193 reviews' '192 reviews' '1,201 reviews' '164 reviews' '1,100 reviews'\n",
      " '388 reviews' '686 reviews' '495 reviews' '469 reviews' '465 reviews'\n",
      " '1,843 reviews' '583 reviews' '3,083 reviews' '294 reviews' '603 reviews'\n",
      " '52 reviews' '334 reviews' '126 reviews' '854 reviews' '496 reviews'\n",
      " '57 reviews' '297 reviews' '453 reviews' '63 reviews' '105 reviews'\n",
      " '338 reviews' '145 reviews' '923 reviews' '41 reviews' '291 reviews'\n",
      " '436 reviews' '98 reviews' '597 reviews' '127 reviews' '38 reviews'\n",
      " '705 reviews' '2,580 reviews' '482 reviews' '415 reviews' '410 reviews'\n",
      " '2,057 reviews' '340 reviews' '837 reviews' '1,140 reviews' '434 reviews'\n",
      " '972 reviews' '97 reviews' '101 reviews' '766 reviews' '541 reviews'\n",
      " '1,454 reviews' '425 reviews' '227 reviews' '309 reviews' '1,741 reviews'\n",
      " '320 reviews' '124 reviews' '288 reviews' '701 reviews' '777 reviews'\n",
      " '78 reviews' '489 reviews' '325 reviews' '257 reviews' '760 reviews'\n",
      " '576 reviews' '448 reviews' '430 reviews' '439 reviews' '957 reviews'\n",
      " '18 reviews' '1,089 reviews' '643 reviews' '331 reviews' '92 reviews'\n",
      " '1,277 reviews' '195 reviews' '506 reviews' '1,038 reviews' '318 reviews'\n",
      " '532 reviews' '368 reviews' '1,215 reviews' '212 reviews' '79 reviews'\n",
      " '196 reviews' '7 reviews' '1,216 reviews' '31 reviews' '183 reviews'\n",
      " '619 reviews' '81 reviews' '522 reviews' '412 reviews' '585 reviews'\n",
      " '275 reviews' '333 reviews' '300 reviews' '365 reviews' '67 reviews'\n",
      " '1,191 reviews' '708 reviews' '237 reviews' '344 reviews' '1,104 reviews'\n",
      " '1,605 reviews' '710 reviews' '1,088 reviews' '817 reviews' '33 reviews'\n",
      " '277 reviews' '213 reviews' '512 reviews' '943 reviews' '337 reviews'\n",
      " '155 reviews' '454 reviews' '995 reviews' '360 reviews' '367 reviews'\n",
      " '376 reviews' '14 reviews' '501 reviews' '530 reviews' '466 reviews'\n",
      " '811 reviews' '743 reviews' '908 reviews' '289 reviews' '359 reviews'\n",
      " '259 reviews' '630 reviews' '963 reviews' '1,331 reviews' '143 reviews'\n",
      " '1,732 reviews' '286 reviews' '461 reviews' '279 reviews' '671 reviews'\n",
      " '1,111 reviews' '268 reviews' '492 reviews' '361 reviews' '661 reviews'\n",
      " '1,103 reviews' '389 reviews' '199 reviews' '1,762 reviews' '570 reviews'\n",
      " '483 reviews' '390 reviews' '497 reviews' '823 reviews' '588 reviews'\n",
      " '343 reviews' '414 reviews' '93 reviews' '517 reviews' '346 reviews'\n",
      " '2,071 reviews' '1,047 reviews' '884 reviews' '700 reviews' '423 reviews'\n",
      " '335 reviews' '348 reviews' '567 reviews' '1,242 reviews' '794 reviews'\n",
      " '754 reviews' '3,978 reviews' '1,049 reviews' '347 reviews'\n",
      " '1,766 reviews' '3,113 reviews' '2,204 reviews' '724 reviews'\n",
      " '502 reviews' '391 reviews' '514 reviews' '713 reviews' '715 reviews'\n",
      " '1,177 reviews' '674 reviews' '579 reviews' '1,541 reviews' '409 reviews'\n",
      " '470 reviews' '2,750 reviews' '381 reviews' '1,018 reviews' '261 reviews'\n",
      " '2,987 reviews' '487 reviews' '416 reviews' '676 reviews' '397 reviews'\n",
      " '970 reviews' '1,584 reviews' '427 reviews' '886 reviews' '122 reviews'\n",
      " '919 reviews' '663 reviews' '168 reviews' '5,448 reviews' '891 reviews'\n",
      " '408 reviews' '476 reviews' '519 reviews' '295 reviews' '442 reviews'\n",
      " '818 reviews' '820 reviews' '1,520 reviews' '17 reviews' '1,283 reviews'\n",
      " '732 reviews' '445 reviews' '371 reviews' '358 reviews' '440 reviews'\n",
      " '776 reviews' '328 reviews' '446 reviews' '1,055 reviews' '1,922 reviews'\n",
      " '385 reviews' '250 reviews' '951 reviews' '620 reviews' '859 reviews'\n",
      " '422 reviews' '1,223 reviews' '339 reviews' '894 reviews' '2,676 reviews'\n",
      " '405 reviews' '965 reviews' '1,274 reviews' '330 reviews' '1,858 reviews'\n",
      " '918 reviews' '1,197 reviews' '559 reviews' '757 reviews' '692 reviews'\n",
      " '420 reviews' '3,444 reviews' '640 reviews' '1,063 reviews' '741 reviews'\n",
      " '526 reviews' '245 reviews' '1,333 reviews' '614 reviews' '1,599 reviews'\n",
      " '1,094 reviews' '1,342 reviews' '575 reviews' '616 reviews' '941 reviews'\n",
      " '4,055 reviews' '714 reviews' '352 reviews' '494 reviews' '1,003 reviews'\n",
      " '1,669 reviews' '387 reviews' '631 reviews' '382 reviews' '774 reviews'\n",
      " '572 reviews' '621 reviews' '1,026 reviews' '910 reviews' '599 reviews'\n",
      " '952 reviews' '546 reviews' '431 reviews' '625 reviews' '1,537 reviews'\n",
      " '990 reviews' '208 reviews' '764 reviews' '991 reviews' '479 reviews'\n",
      " '711 reviews' '826 reviews' '565 reviews' '296 reviews' '608 reviews']\n"
     ]
    }
   ],
   "source": [
    "# Find unique values for No_of_Reviews column \n",
    "print(f'No_of_Reviews column has:{len(df_new_dataset.No_of_Reviews.unique())} data')\n",
    "print(\"Unique values of \"'No_of_Reviews'\" column:\")\n",
    "print(df_new_dataset.No_of_Reviews.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d81f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase readability, we remove redundent words from No_of_Reviews column\n",
    "df_new_dataset['No_of_Reviews'] = [n.split()[0].replace(',', '') for n in df_new_dataset['No_of_Reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb6339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unrelateable data exist in No_of_Reviews column\n"
     ]
    }
   ],
   "source": [
    "# As No_of_Reviews column has 810 data, we can't review all unique values in bare eyes. \n",
    "# Therefore, we check closely No_of_Reviews column to find unrelateable string data\n",
    "is_no_of_reviews = False\n",
    "for review in df_new_dataset.No_of_Reviews:    \n",
    "    if review == 'No review' or review == 'Undefined':\n",
    "        is_no_of_reviews = True\n",
    "        print(review)\n",
    "    else:\n",
    "        pass\n",
    "if is_no_of_reviews:\n",
    "    print(\"Unrelatable data exists\")\n",
    "else:\n",
    "    print(\"No unrelateable data exist in No_of_Reviews column\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00d50114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price_Range column has: 3 data\n",
      "Unique values of Price_Range column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['$$ - $$$', '$$$$', '$'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find unique values for Price_Range column to see there is any non-meaningful string exist\n",
    "print(f'Price_Range column has: {len(df_new_dataset.Price_Range.unique())} data')\n",
    "print(\"Unique values of \"'Price_Range'\" column:\")\n",
    "df_new_dataset.Price_Range.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04543590",
   "metadata": {},
   "source": [
    "- Assign descriptive values for price range to make the data more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b92aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Price_Range column's value\n",
    "def price_range(price_value):\n",
    "    if price_value == '$$ - $$$':\n",
    "        return  \"Medium\"\n",
    "    elif price_value == '$$$$':\n",
    "        return  \"Expensive\"\n",
    "    else:\n",
    "        return  \"Cheap\"    \n",
    "\n",
    "df_new_dataset['Price_Range'] = df_new_dataset['Price_Range'].apply(price_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30fc8d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Medium', 'Expensive', 'Cheap'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_dataset.Price_Range.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e7511",
   "metadata": {},
   "source": [
    "- As we will work with states of USA, so we will derive `state`, `city` and `Zip_Code`  from `location` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cb3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f8c7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive City from Location\n",
    "def city_location(location):\n",
    "    return location.split(',')[0].strip()\n",
    "df_new_dataset['City'] = df_new_dataset['Location'].apply(city_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7781db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of cities: 161\n",
      "All cities: ['Hackensack' 'Poughkeepsie' 'Plano' 'Syracuse' 'Federal Way' 'Dallas'\n",
      " 'Renton' 'Ithaca' 'Vancouver' 'Kirkland' 'Albany' 'Montclair' 'Princeton'\n",
      " 'Bellevue' 'Saratoga Springs' 'Frisco' 'Atlantic City' 'Bothell'\n",
      " 'Seattle' 'Teaneck' 'Staten Island' 'Toms River' 'Laredo' 'Amarillo'\n",
      " 'New York City' 'Freehold' 'Austin' 'Yonkers' 'Fort Worth' 'Waco'\n",
      " 'Kennewick' 'Ship Bottom' 'El Paso' 'Forest Hills' 'Puyallup'\n",
      " 'Bellingham' 'Spokane Valley' 'Kent' 'Cherry Hill' 'New Brunswick' 'Katy'\n",
      " 'Everett' 'The Woodlands' 'Cedar Grove' 'Elizabeth' 'South Ozone Park'\n",
      " 'Newark' 'Richardson' 'Lubbock' 'Redmond' 'Ocean City' 'Niagara Falls'\n",
      " 'Yakima' 'Brownsville' 'Corpus Christi' 'Irving' 'Houston' 'Spring'\n",
      " 'Portland' 'Brick' 'Spokane' 'San Antonio' 'Highland Park' 'Tacoma'\n",
      " 'Wayne' 'Schenectady' 'Flushing' 'Astoria' 'Long Island City' 'Olympia'\n",
      " 'Brooklyn' 'Arlington' 'Jersey City' 'Lynnwood' 'McAllen' 'Stanwood'\n",
      " 'Coupeville' 'Beach Haven' 'Woodland Park' 'East Elmhurst' 'Bronx'\n",
      " 'Jamaica' 'Pharr' 'Paterson' 'Ewing' 'Barnegat Light' 'Metuchen'\n",
      " 'Jackson Heights' 'Fair Lawn' 'Trenton' 'Edison' 'Buffalo' 'Yardley'\n",
      " 'West Orange' 'White Plains' 'Howard Beach' 'Clifton' 'Elmwood Park'\n",
      " 'Hawthorne' 'Rensselaer' 'Sunnyside' 'Garland' 'Mukilteo' 'Somerset'\n",
      " 'Fort Lee' 'Totowa' 'Rochester' 'Hasbrouck Heights' 'Corona' 'Hamilton'\n",
      " 'Woodside' 'Bordentown' 'Oak Harbor' 'Morrisville' 'Bronxville'\n",
      " 'East Newark' 'Langley' 'Westmont' 'Surf City' 'Maywood' 'Lawrenceville'\n",
      " 'Bloomfield' 'South Hackensack' 'Rochelle Park' 'Mill Creek' 'Marysville'\n",
      " 'Rego Park' 'Teterboro' 'Port Townsend' 'Harrison' 'Tenafly' 'Edgewater'\n",
      " 'Harvey Cedars' 'Lakewood' 'Hoboken' 'Edmonds' 'Beachwood' 'Addison'\n",
      " 'Kew Gardens' 'Lincoln Park' 'Long Beach Township' 'Bayville' 'Nutley'\n",
      " 'Ozone Park' 'Elmhurst' 'Freeland' 'Paramus' 'New Milford' 'Orange'\n",
      " 'Collingswood' 'Passaic' 'Haddonfield' 'Haledon' 'Cresskill' 'Glendale'\n",
      " 'Fairfield' 'Linden' 'Pompton Plains' 'Bogota' 'Piscataway' 'Bayside']\n"
     ]
    }
   ],
   "source": [
    "print(f'Total no. of cities: {len(df_new_dataset.City.unique())}')\n",
    "print(f'All cities: {df_new_dataset.City.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d22a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive State from Location\n",
    "def state_location(location):\n",
    "        if len(location.split(',')) == 2:\n",
    "            return location.split(',')[1][:3].strip() \n",
    "        else:\n",
    "            return location.split(',')[2][:3].strip() \n",
    "        \n",
    "df_new_dataset['State'] = df_new_dataset['Location'].apply(state_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00cd5dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of states: 7\n",
      "All states: ['NJ' 'NY' 'TX' 'WA' 'OR' 'PA' '']\n"
     ]
    }
   ],
   "source": [
    "print(f'Total no. of states: {len(df_new_dataset.State.unique())}')\n",
    "print(f'All states: {df_new_dataset.State.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b680a",
   "metadata": {},
   "source": [
    "- `CA` (California) state droped out from the dataset as we droped `NaN` values from `Type` and `comments` Columns. We have five different states of USA named `New Jersy`, `New York`, `Texas`, `Washington`, `Oregon` and `Pennsylvania`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b86571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new_dataset[df_new_dataset['State'] == \"CA\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4be97a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Street_Address</th>\n",
       "      <th>Location</th>\n",
       "      <th>Type</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Price_Range</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>The Watermark</td>\n",
       "      <td>6361 Fallsview Blvd</td>\n",
       "      <td>Niagara Falls, Ontario L2G 3V9,</td>\n",
       "      <td>American, Canadian, Contemporary</td>\n",
       "      <td>4</td>\n",
       "      <td>1342</td>\n",
       "      <td>Our dining experience at The Watermark this pa...</td>\n",
       "      <td>Expensive</td>\n",
       "      <td>Niagara Falls</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name       Street_Address                          Location  \\\n",
       "2617  The Watermark  6361 Fallsview Blvd  Niagara Falls, Ontario L2G 3V9,    \n",
       "\n",
       "                                   Type Reviews No_of_Reviews  \\\n",
       "2617   American, Canadian, Contemporary       4          1342   \n",
       "\n",
       "                                               Comments Price_Range  \\\n",
       "2617  Our dining experience at The Watermark this pa...   Expensive   \n",
       "\n",
       "               City State  \n",
       "2617  Niagara Falls        "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check empty string in State column\n",
    "df_new_dataset[df_new_dataset['State'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5b1bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty string in State column\n",
    "df_new_dataset.drop(df_new_dataset[(df_new_dataset['State'] == '')].index, inplace=True)\n",
    "# df_new_dataset = df_new_dataset.drop(['Location'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9082eae",
   "metadata": {},
   "source": [
    "- We droped the row that contain `province of Canada` as we will work with states of USA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4452c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Zip_Code from Location\n",
    "def zipcode_location(zipcode):\n",
    "    return zipcode.split(\",\")[-1].split(\" \")[-1]\n",
    "        \n",
    "df_new_dataset['Zip_Code'] = df_new_dataset['Location'].apply(zipcode_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "139178eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Location column\n",
    "df_new_dataset = df_new_dataset.drop(['Location'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dcb80",
   "metadata": {},
   "source": [
    "We also droped the `Location` column as we got our desired column `City`, `State` and `Zip_Code` from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8c1f7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2439 entries, 1 to 3061\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Name            2439 non-null   object\n",
      " 1   Street_Address  2439 non-null   object\n",
      " 2   Type            2439 non-null   object\n",
      " 3   Reviews         2439 non-null   object\n",
      " 4   No_of_Reviews   2439 non-null   object\n",
      " 5   Comments        2439 non-null   object\n",
      " 6   Price_Range     2439 non-null   object\n",
      " 7   City            2439 non-null   object\n",
      " 8   State           2439 non-null   object\n",
      " 9   Zip_Code        2439 non-null   object\n",
      "dtypes: object(10)\n",
      "memory usage: 209.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dataset info droping all NaN values and non required row(s) and column(s)\n",
    "df_new_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ef04f",
   "metadata": {},
   "source": [
    "- Convert `Review` column's data to `float` and `No_of_Reviews` column's data to `int`. And reorder the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4ff076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review column's data to float\n",
    "df_new_dataset['Reviews'] = df_new_dataset['Reviews'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe761bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert No_of_Reviews column's data to int\n",
    "df_new_dataset['No_of_Reviews'] = df_new_dataset['No_of_Reviews'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55fee541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering columns\n",
    "df_new_dataset = df_new_dataset.loc[:,['Name', 'Street_Address', 'State', 'City', 'Zip_Code', 'Type', 'Reviews', 'No_of_Reviews', 'Comments', 'Price_Range']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05732aca",
   "metadata": {},
   "source": [
    "- Reset index as we dropped some columns and rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d57e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic info of dataset:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2439 entries, 0 to 2438\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Name            2439 non-null   object \n",
      " 1   Street_Address  2439 non-null   object \n",
      " 2   State           2439 non-null   object \n",
      " 3   City            2439 non-null   object \n",
      " 4   Zip_Code        2439 non-null   object \n",
      " 5   Type            2439 non-null   object \n",
      " 6   Reviews         2439 non-null   float64\n",
      " 7   No_of_Reviews   2439 non-null   int64  \n",
      " 8   Comments        2439 non-null   object \n",
      " 9   Price_Range     2439 non-null   object \n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 190.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Reset index of dataset\n",
    "df_new_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# View dataset's overall basic info by per columns\n",
    "print('Basic info of dataset:\\n') \n",
    "df_new_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f23c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data set to .csv format\n",
    "df_new_dataset.to_csv('TripAdvisor_RestauarantRecommendation_cleandataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
